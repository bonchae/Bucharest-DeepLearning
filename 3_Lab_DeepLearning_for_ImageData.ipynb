{"cells":[{"cell_type":"markdown","id":"0bBMDURcx1rv","metadata":{"id":"0bBMDURcx1rv"},"source":["# Deep Learning for Image Data\n","\n","- Pre-trained models\n","- Model building from scratch"]},{"cell_type":"markdown","source":["# Pre-trained models"],"metadata":{"id":"t5nKUDT4_Bfs"},"id":"t5nKUDT4_Bfs"},{"cell_type":"markdown","source":["<img src=\"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/cecbccba-6358-476e-9fd8-e2807de9f220/Frame_118.png?t=1693044751\" width=500>\n","\n","Founded in 2016\n","\n","Thousands of models (e.g., BERT, ChatGPT) you can use **without training from scratch**!\n","\n","[Go to Hugging Face](https://huggingface.co/) and explore [the pre-trained models available on the website](https://huggingface.co/models)."],"metadata":{"id":"CO-AiZk5_f05"},"id":"CO-AiZk5_f05"},{"cell_type":"markdown","source":["## [ResNet-50 v1.5](https://huggingface.co/microsoft/resnet-50)\n","\n","\"ResNet (Residual Network) is a convolutional neural network. ResNet model pre-trained on ImageNet-1k at resolution 224x224.\"\n","\n","- 1,000 object categories (classes)\n","- 1.2 million training images\n","- 50,000 validation images"],"metadata":{"id":"2Ka7MKb0_ms-"},"id":"2Ka7MKb0_ms-"},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from transformers import pipeline\n","import torch\n","\n","import textwrap # print output in multiple lines"],"metadata":{"id":"6o5KGURv-DDO"},"id":"6o5KGURv-DDO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg?crop=1.00xw:0.756xh;0,0.134xh&resize=1024:\">"],"metadata":{"id":"Yh_qco10_wjr"},"id":"Yh_qco10_wjr"},{"cell_type":"code","source":["# Check for CUDA (GPU)\n","device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n","print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n","\n","# Load image classification pipeline with ResNet-50 (CNN)\n","classifier = pipeline(\n","    \"image-classification\",\n","    model=\"microsoft/resnet-50\",\n","    device=device,\n","    use_fast=True  # Use the fast image processor to avoid the warning\n",")\n","\n","# Classify the image\n","result = classifier(\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg\")\n","\n","for item in result:\n","    print(f\"Label: {item['label']}, Score: {item['score']:.4f}\")"],"metadata":{"id":"-kNYbN6U-_WW"},"id":"-kNYbN6U-_WW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\">"],"metadata":{"id":"1iAnR2di_2BR"},"id":"1iAnR2di_2BR"},{"cell_type":"code","source":["from transformers import BlipProcessor, BlipForConditionalGeneration\n","from PIL import Image\n","import requests\n","\n","image = Image.open(requests.get(\"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\", stream=True).raw)\n","\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","\n","inputs = processor(image, return_tensors=\"pt\")\n","out = model.generate(**inputs)\n","print(processor.decode(out[0], skip_special_tokens=True))\n","\n","\n"],"metadata":{"id":"LqRCYixi-_ZQ"},"id":"LqRCYixi-_ZQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["btt? not BTS?\n","\n","Oops! That looks like a hallucination from the model.\n","\n","Advanced models (e.g., [BLIP 2](https://huggingface.co/docs/transformers/en/model_doc/blip-2)) are more accurate."],"metadata":{"id":"y1lcBi5PAP5n"},"id":"y1lcBi5PAP5n"},{"cell_type":"markdown","source":["## [CLIP model](https://huggingface.co/docs/transformers/en/model_doc/clip)\n","\n","\"CLIP is a is a multimodal vision and language model motivated by **overcoming the fixed number of object categories** when training a computer vision model. CLIP learns about images directly from raw text by jointly training on 400M (image, text) pairs. Pretraining on this scale enables **zero-shot transfer** to downstream tasks.\" Developed by the OpenAI organization.\n","\n","This is a **transformer**-based model."],"metadata":{"id":"65D-bhkg_8uE"},"id":"65D-bhkg_8uE"},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import torch, requests\n","\n","# Load model & processor\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","\n","# Image and candidate captions\n","image = Image.open(requests.get(\n","    \"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\",\n","    stream=True).raw)\n","\n","texts = [\"a photo of BTS\",\n","         \"a photo of a dog\",\n","         \"a photo of a band\",\n","         \"a group of men\"]\n","\n","# Predict\n","inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n","probs = model(**inputs).logits_per_image.softmax(dim=1)[0]\n","\n","# Display results\n","print(\"\\n CLIP Similarity Scores:\")\n","for text, p in zip(texts, probs):\n","    print(f\"{text:<25} -> {p:.4f}\")"],"metadata":{"id":"QgocR3pL-_gg"},"id":"QgocR3pL-_gg","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"qn2UY66ryghY","metadata":{"id":"qn2UY66ryghY"},"source":["# CNN Model Building from Scratch"]},{"cell_type":"markdown","source":["<img src=\"https://i0.wp.com/developersbreach.com/wp-content/uploads/2020/08/cnn_banner.png?fit=1400%2C658&ssl=1\">"],"metadata":{"id":"51ChB-vd_OHF"},"id":"51ChB-vd_OHF"},{"cell_type":"markdown","source":["### CNN Architecture Summary Table\n","\n","| Step | Layer Type               | Description                                                                 |\n","|------|--------------------------|-----------------------------------------------------------------------------|\n","| 1️⃣   | **Input**                | Raw image input (e.g., a zebra).                                            |\n","| 2️⃣   | **Convolution + ReLU**  | Filters (kernels) extract features like edges; ReLU adds non-linearity.     |\n","| 3️⃣   | **Pooling**             | Downsamples feature maps to reduce size and retain important info.          |\n","| 4️⃣   | **Convolution + ReLU**  | Further feature extraction (deeper patterns).                               |\n","| 5️⃣   | **Pooling**             | More downsampling for dimensionality reduction.                             |\n","| 6️⃣   | **Flatten**             | Converts feature maps into a 1D feature vector.                             |\n","| 7️⃣   | **Fully Connected**     | Dense layers combine features and learn decision boundaries.                |\n","| 8️⃣   | **Output (Softmax)**    | Outputs class probabilities (e.g., Zebra: 0.7).                             |\n","\n","✅ **Final Prediction**: Class with highest probability (e.g., **Zebra**).\n"],"metadata":{"id":"ZSJ8Xsw3tK7y"},"id":"ZSJ8Xsw3tK7y"},{"cell_type":"markdown","source":["```python\n","model = tf.keras.Sequential([\n","    tf.keras.Input(shape=(28, 28, 1)),\n","    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n"],"metadata":{"id":"36Ll51Op52m7"},"id":"36Ll51Op52m7"},{"cell_type":"markdown","source":["**CNN Model Layer Breakdown**\n","\n","| Layer                                      | Purpose (Matches CNN Diagram)                                               |\n","|-------------------------------------------|------------------------------------------------------------------------------|\n","| `tf.keras.Input(shape=(28,28,1))`          | Input layer for 28×28 grayscale images (e.g., MNIST digits)                 |\n","| `Conv2D(16, (3, 3), activation='relu')`    | Convolution layer + ReLU to extract local patterns                          |\n","| `MaxPooling2D(2, 2)`                       | Pooling layer to downsample and retain key features                         |\n","| `Flatten()`                                | Flatten feature maps to 1D vector (prepares for Dense layers)               |\n","| `Dense(64, activation='relu')`             | Fully connected hidden layer                                                |\n","| `Dense(10, activation='softmax')`          | Output layer: softmax to predict probabilities for 10 classes               |\n"],"metadata":{"id":"KuQusdtctnVk"},"id":"KuQusdtctnVk"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","\n","# Deep Learning Setup\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential           # Sequential model: stack layers linearly\n","from tensorflow.keras.layers import Dense, Input         # Dense: fully connected layer, Input: define input shape\n","from tensorflow.keras.optimizers import Adam             # Adam: an efficient optimizer for training\n","from tensorflow.keras.utils import plot_model\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Set seeds for reproducibility\n","import random\n","seed_value = 42  # Choose any seed value you want\n","random.seed(seed_value)\n","np.random.seed(seed_value)\n","tf.random.set_seed(seed_value)\n","tf.config.experimental.enable_op_determinism()  # TensorFlow 2.9+"],"metadata":{"id":"XgVbQuYP-EK7"},"id":"XgVbQuYP-EK7","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"Rb_Q8I38CGQD","metadata":{"id":"Rb_Q8I38CGQD"},"outputs":[],"source":["# Load data\n","(images, labels), _ = tf.keras.datasets.fashion_mnist.load_data()\n","\n","# images are like X.\n","# labels are like y."]},{"cell_type":"markdown","id":"WPQ1ylgREbT6","metadata":{"id":"WPQ1ylgREbT6"},"source":["Fashion MNIST is a dataset of **grayscale images** of clothing items, commonly used for training image classification models.\n","\n","- **Training set**: 60,000 images and labels\n","- **Test set**: 10,000 images and labels\n","- **Image size**: 28 × 28 pixels\n","- **Color**: Grayscale (single channel)\n","- **Labels**: Integers from 0 to 9, each representing a clothing category\n","\n","| Label | Class Name   |\n","|------|--------------|\n","| 0    | T-shirt/top   |\n","| 1    | Trouser       |\n","| 2    | Pullover      |\n","| 3    | Dress         |\n","| 4    | Coat          |\n","| 5    | Sandal        |\n","| 6    | Shirt         |\n","| 7    | Sneaker       |\n","| 8    | Bag           |\n","| 9    | Ankle boot    |\n","\n","Each image represents one article of clothing, and the label indicates the correct category.\n"]},{"cell_type":"code","source":["# Create a DataFrame\n","\n","flat_images = images.reshape(images.shape[0], -1)\n","\n","df = pd.DataFrame(flat_images)\n","df['label'] = labels\n","\n","label_map = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',\n","    5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n","df['label_name'] = df['label'].map(label_map)\n","\n","df.head()"],"metadata":{"id":"mz3Y3hxTwuGs"},"id":"mz3Y3hxTwuGs","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"m4IJD9v9Fpbi","metadata":{"id":"m4IJD9v9Fpbi"},"outputs":[],"source":["# view the first image\n","plt.imshow(images[0], cmap='gray')\n","plt.title(\"Label: {}\".format(labels[0]))\n","plt.show()\n","# Label 9 is Ankle boot"]},{"cell_type":"code","execution_count":null,"id":"xIHy-vQcFI2i","metadata":{"id":"xIHy-vQcFI2i"},"outputs":[],"source":["# view the actual value of the above image\n","images[0]"]},{"cell_type":"markdown","id":"jOHKdl25FjnG","metadata":{"id":"jOHKdl25FjnG"},"source":["- 28 rows  → height of the image  \n","- 28 cols  → width of the image  \n","- Each number = brightness of a pixel"]},{"cell_type":"code","execution_count":null,"id":"6efeLjGBCY6i","metadata":{"id":"6efeLjGBCY6i"},"outputs":[],"source":["# In training ML/DL models, we normalize numerical values (actual values to the range between 0 and 1)\n","# Normalize pixel values from 0–255 ==> 0.0–1.0\n","images = images / 255.0"]},{"cell_type":"code","execution_count":null,"id":"S_EkfZLbIQXG","metadata":{"id":"S_EkfZLbIQXG"},"outputs":[],"source":["# Expected shape by a CNN: (height, width, channels)\n","# Must be 3D per image: (28, 28, 1)\n","# The 1 is the channel → 1 for grayscale, 3 for RGB.\n","# Add channel dimension (needed for CNN)\n","images = images.reshape(-1, 28, 28, 1)"]},{"cell_type":"markdown","source":["| Value | Meaning |\n","|:------|:--------|\n","| -1 | Automatically infer the number of images (batch size) |\n","| 28 | Height (pixels) |\n","| 28 | Width (pixels) |\n","| 1 | 1 Channel (grayscale) |\n"],"metadata":{"id":"Ws78OSrd6vsE"},"id":"Ws78OSrd6vsE"},{"cell_type":"markdown","source":["## Training model"],"metadata":{"id":"YxwEK85X-R3c"},"id":"YxwEK85X-R3c"},{"cell_type":"code","execution_count":null,"id":"xT8G17maHNqh","metadata":{"id":"xT8G17maHNqh"},"outputs":[],"source":["# Build a simple CNN model with softmax output\n","\n","model = tf.keras.Sequential([\n","    tf.keras.Input(shape=(28, 28, 1)),                    # Input: grayscale image\n","    tf.keras.layers.Conv2D(16, (3, 3), activation=''),    # 'relu' - Conv layer to extract patterns\n","    tf.keras.layers.MaxPooling2D(2, 2),                    # Downsample by 2\n","    tf.keras.layers.Flatten(),                             # Flatten 2D → 1D\n","    tf.keras.layers.Dense(64, activation=''),              # 'relu' - Hidden layer\n","    tf.keras.layers.Dense(10, activation='')        # Output: 'softmax' - 10 class probabilities\n","])"]},{"cell_type":"code","execution_count":null,"id":"H7J6MwjhHNnw","metadata":{"id":"H7J6MwjhHNnw"},"outputs":[],"source":["# Choose the optimizer, loss function, and metric:\n","model.compile(\n","    optimizer='',\n","    loss='',      # used for multi-class classification (e.g., classifying images into categories: cat, dog, car, airplane, etc.)\n","    metrics=['']\n",")"]},{"cell_type":"code","execution_count":null,"id":"GDvVZW4BHNtT","metadata":{"id":"GDvVZW4BHNtT"},"outputs":[],"source":["# Train the model (using all data, no split)\n","history = model.fit( , , epochs=3)"]},{"cell_type":"markdown","id":"pE3FLoupTTBO","metadata":{"id":"pE3FLoupTTBO"},"source":["Learning Progress Over Epochs\n","\n","| Epoch  | What Happens                             | Accuracy     |\n","|--------|-------------------------------------------|--------------|\n","| 1️⃣     | Model starts with random weights          | Low          |\n","| 2️⃣     | Learns basic patterns                     | Higher       |\n","| 3️⃣ | Learns finer patterns, reduces mistakes   | Even higher  |\n"]},{"cell_type":"code","execution_count":null,"id":"OtRyc4Hg9KFs","metadata":{"id":"OtRyc4Hg9KFs"},"outputs":[],"source":["# Plot training history\n","plt.plot(history.history['accuracy'], label='Train Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Training Accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"TF4_jF4VHNwP","metadata":{"id":"TF4_jF4VHNwP"},"outputs":[],"source":["# Accuracy after each epoch\n","print(history.history['accuracy'])\n","\n","# Final accuracy\n","final_acc = history.history['accuracy'][-1]\n","print(f\"Final Training Accuracy: {final_acc:.2f}\")"]},{"cell_type":"code","execution_count":null,"id":"XiZ5yM-jL9gE","metadata":{"id":"XiZ5yM-jL9gE"},"outputs":[],"source":["probabilities = model.predict(images)\n","\n","# Convert to class labels\n","y_pred = np.argmax(probabilities, axis=1)\n","\n","# True labels\n","y_true = labels  # still integers 0–9\n","\n","cm = confusion_matrix(y_true, y_pred)\n","cm"]},{"cell_type":"markdown","source":["Looks like predicting **label 6 Shirt** is difficult."],"metadata":{"id":"Fia-AVDwzKl0"},"id":"Fia-AVDwzKl0"},{"cell_type":"code","source":["# Find the first index where label == 6 ('Shirt')\n","shirt_index = (labels == 6).nonzero()[0][0]\n","\n","# Get the image\n","shirt_image = images[shirt_index]\n","\n","# Plot the image\n","plt.imshow(shirt_image, cmap='gray')\n","plt.title('Label: 6 (Shirt)')\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"t0m7RQFszIqc"},"id":"t0m7RQFszIqc","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2o4lgv9lKk7o","metadata":{"id":"2o4lgv9lKk7o"},"source":["## Predict New Images"]},{"cell_type":"code","execution_count":null,"id":"_I6vrkfbKnYc","metadata":{"id":"_I6vrkfbKnYc"},"outputs":[],"source":["# This is the second image in the dataset. It's a T-shirt/top and its label is 0\n","\n","sample = images[1]  # already normalized, shape: (28, 28, 1)\n","\n","plt.imshow(sample, cmap='gray')\n","plt.title(\"A sample image. Predict me!\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"uMZMoao5Knbh","metadata":{"id":"uMZMoao5Knbh"},"outputs":[],"source":["sample = sample.reshape(1, 28, 28, 1)"]},{"cell_type":"code","execution_count":null,"id":"U3xaw5N3Kndz","metadata":{"id":"U3xaw5N3Kndz"},"outputs":[],"source":["probabilities = model.predict(sample)\n","predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]\n","print(f\"Predicted class: {predicted_class}\")\n","\n","# Labe 0 is a T-shirt/top"]},{"cell_type":"markdown","source":["Let's try one more."],"metadata":{"id":"54GyJx14zkGv"},"id":"54GyJx14zkGv"},{"cell_type":"code","source":["# Label 6 - Shirt\n","print(shirt_index)"],"metadata":{"id":"bf1FffeezdXf"},"id":"bf1FffeezdXf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = images[18]\n","\n","plt.imshow(sample, cmap='gray')\n","plt.title(\"A sample image. Predict me!\")\n","plt.show()"],"metadata":{"id":"A2tyvKYPzdaI"},"id":"A2tyvKYPzdaI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = sample.reshape(1, 28, 28, 1)"],"metadata":{"id":"Bo08Nw7Gzq0b"},"id":"Bo08Nw7Gzq0b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["probabilities = model.predict(sample)\n","predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]\n","print(f\"Predicted class: {predicted_class}\")\n","\n","# Labe 6 is a Shirt"],"metadata":{"id":"YA4NfPjNzq35"},"id":"YA4NfPjNzq35","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"_pA7J6zUREmZ","metadata":{"id":"_pA7J6zUREmZ"},"source":["Conclusion: Our image recognization model works well :)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}